---
title: Features
description: Learn about platform features and web crawling philosophies
icon: wand-magic-sparkles
---

## Scraping

### Query selectors

### AI inference

## Compliance

### robots.txt

A website's `/robots.txt` file defines its participation in the [Robots Exclusion Protocol](https://en.wikipedia.org/wiki/Robots.txt). Crawlspace automatically respects the directives set within robots.txt, such as disallowing access to certain paths. This ensures ethical scraping and minimizes risk of legal or reputational repercussions.

### Retries

## Queueing

### Normalization

To avoid redundant requests, Crawlspace normalizes URLs before adding them to the queue. This ensures each URL is processed once, even if encountered multiple times across pages. You can customize the normalization logic to handle URL parameters and edge cases.

### Honeypot avoidance

Crawlspace detects and avoids honeypots—traps set by websites to identify crawlers—using heuristics such as unusual link patterns or hidden elements. Configurable strategies allow you to tune sensitivity for your use case.

## Performance

### Concurrency

Concurrency is the ability of a system to handle multiple tasks or operations at the same time.

Crawlspace supports high-concurrency crawling, allowing you to process multiple requests simultaneously. Concurrency settings are adjustable to balance speed with respect for target server capacity, helping maintain ethical crawling practices.

### Scaling

Scaling is the process of increasing the system’s capacity to handle a growing workload, either by adding more resources or optimizing existing ones.

Whether you’re running a single crawler or managing hundreds, Crawlspace scales seamlessly. Horizontal scaling allows you to distribute workloads across multiple instances, while internal optimizations ensure high performance on resource-constrained environments.

## Storage

Structured data scraped by your crawler is stored in a SQLite database. Every user account on Crawlspace gets their own dedicated database, and every crawler gets its own table within that database. 

### Inserts

### Upserts

### Migrations

### Exporting

### Metadata

## Scheduling

### Cron syntax

Crawlspace uses cron expressions for scheduling recurring tasks. For example, a cron like 0 0 * * * lets you run a crawler daily at midnight. Detailed validation ensures your expressions are correctly formatted before scheduling jobs.

## Proxies

*Coming Soon!*

Crawlspace will soon support proxy configurations to enable geo-specific scraping, anonymity, and better compliance with rate-limiting policies.

## JavaScript evaluation

*Coming Soon!*

Run headless browser instances to evaluate and interact with JavaScript-heavy pages. This will enable accurate scraping of SPAs, dynamic content, and interactive elements.
